---
title: >
    Apple and the AI divide
date: 2025-01-07 16:22:19 -0400
---

This morning I read a [404 Media article](https://www.404media.co/instagram-begins-randomly-showing-users-ai-generated-images-of-themselves/) about Instagram showing people ads with AI-generated images of themselves. I thought [this take from Sam Biddle](https://bsky.app/profile/sambiddle.com/post/3lf4cvbfzj22z) was very good:

> Never in my career have I seen such a giant gulf between *What Companies Think Is the Most Important Thing in the World* and *What Normal People Have Absolutely Any Interest in Whatsoever*

Meta has always been a taste-free zone, so this sort of promotion isn't surprising to me. Apple, who have always tried to embody taste, are also tripping over themselves to squeeze AI into everything. I think some of their ideas are reasonable, but [not all of them](https://support.apple.com/en-ca/guide/iphone/iph0063238b5/ios). Most recently their AI summarization feature has [tripped up a second time while editing notifications from the BBC](https://www.theverge.com/2025/1/6/24337681/apple-intelligence-summary-bbc-news-luigi-mangione-response).

I think LLMs and generative AI are interesting and useful pieces of technology. I also think they're massively overhyped and their current capabilities are poorly understood. Some people compare the LLM craze to the crypto/blockchain boom, and I think that's unfair. Blockchains are slow, expensive databases masquerading as a social revolution while functionally being a get-rich-quick scheme. LLMs have been useful for years, and are only getting more useful with time. Still, that doesn't mean they should be shoved into every corner of every product.

There's a group of the population that is disgusted by anything related to LLMs or generative AI. Part of this is because LLMs come with a massive ethical issue built-in. Training them requires feeding a statistical model as much content as possible. The makeup of the big LLM training datasets is proprietary, but a conservative bet is that at least 50% of any model's training set is unlicensed copyrighted content. The companies doing the training say that using this copyrighted content is OK because it's [transformative](https://en.wikipedia.org/wiki/Transformative_use). Some of them go so far as to say that [any content on the web is free](https://www.theverge.com/2024/6/28/24188391/microsoft-ai-suleyman-social-contract-freeware).

There's a deep sense of unfairness at play. OpenAI, for example, slurps up content which doesn't belong to them, uses it to get [billions in funding](https://techcrunch.com/2024/10/02/openai-raises-6-6b-and-is-now-valued-at-157b/), and [announces a $200/month subscription plan](https://openai.com/index/introducing-chatgpt-pro/). Meanwhile, regular folks regularly get [smacked around by lawyers](https://www.theverge.com/games/24272743/nintendo-retro-game-corps-russ-crandall-profile-youtube-emulation-dmca-takedown-copyright-strike) for doing similarly transformative stuff. Heck, [the Internet Archive isn't even allowed to lend books](https://en.wikipedia.org/wiki/Hachette_v._Internet_Archive), which I think is a very reasonable use of copyrighted material! [^1]

Because of this, even useful features that have a hint of "AI" get crapped on. Earlier this week I saw [several posts on Bluesky](https://bsky.app/search?q=https%3A%2F%2Fwww.theregister.com%2F2025%2F01%2F03%2Fapple_enhanced_visual_search%2F) linking to [an article on The Register](https://www.theregister.com/2025/01/03/apple_enhanced_visual_search/) about an Apple Photos feature which detects landmarks, and which arrived in October. Many were upset about being automatically opted into something AI related. To me, this feature seems legitimately useful and it also seems like Apple has been more than responsible in terms of preserving privacy. It seemed to be a big deal because there's "AI" at play. Only, according to the [research paper](https://machinelearning.apple.com/research/homomorphic-encryption), it's actually using more traditional machine learning and not LLMs. I can maybe see an argument for having the feature to have been opt-in, but I also think it's unreasonable for Apple to have a checkbox for every new feature in every point release. Meanwhile, what The Register is accusing Apple of is actually the sort of thing that Google does at every opportunity.

My point here is that, like a lot of things these days, there seems to be a weird divide. Companies like Apple are currently trying to "AI" everything. Sometimes this makes sense to me, often it doesn't. Meanwhile, there's a chunk of the public that's angrily opposed to anything vaguely AI-flavoured.

There was a great discussion about this on [Upgrade this week](https://www.relay.fm/upgrade/545), which I recommend. Jason and Myke made a great case that Apple needs to be held more accountable when it screws up, and that it's currently being graded on a curve because "LLMs make mistakes". Apple seems to be pressured to play catch-up on AI technology, and I feel like this is being driven by activist shareholders instead of people who are focused on products. Apple has previously been a company to take their time and do things right. Their current AI strategy seems to be announce everything way too early and release some things before they're ready. From the outside it feels like there was a dictate from on-high that everyone needs to drop what they're doing and sprinkle AI everywhere.

Like I said above, I think that generative AI is useful and interesting. I'm also someone who's very interested in product design. Building a product that starts from a technology instead of a user need is ass-backwards. I really hope Apple is a bit more mindful as it continues to roll out future AI features.

---

[^1]: For whatever it's worth, I square this circle personally because I believe that fair use of copyrighted material should be more permissive in general. The fact that large corporations can get away with copyright uses which most citizens can't is an issue with our current implementation of capitalism, and needs to be addressed from the ground up. Companies like OpenAI are playing the game by the current set of rules, and boycotting their services won't end that.